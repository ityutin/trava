{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_breast_cancer()\n",
    "df = pd.DataFrame(dataset['data'], columns=dataset['feature_names'])\n",
    "df['target'] = dataset['target']\n",
    "\n",
    "# we'll need it in 'Splitter' section\n",
    "df['some_redundant_col'] = np.zeros(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining some parts of Trava to start with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's used to log events from different parts of `Trava`. Nothing's fancy here. Just wanted to have a control over all logging operations across Trava's components. You may initialize it without parameters then no logging at all will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trava.logger import TravaLogger\n",
    "from logging import Logger, StreamHandler\n",
    "\n",
    "logger = Logger(name='preved')\n",
    "logger.addHandler(StreamHandler())\n",
    "\n",
    "trava_logger = TravaLogger(logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorer & ResultsHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ResultsHandler` - responsible for processing a model's metrics. You may want to plot them, to log metrics to a console, to save them on a disk etc. Those metrics are requested using scorers ( similar to sklearn scorers that you make using `make_scorer` func ). Those scorers then turn into metrics - calculated values of different types.\n",
    "\n",
    "There are convenient wrappers around `sklearn`'s metrics functions: `sk` and `sk_proba`. Just pass there sklearn's metric and parameters for it as kwargs.\n",
    "\n",
    "Trava lets you define custom metrics to get any information from data or a model. `FitTimeScorer` and `PredictTimeScorer` are examples of such metrics. \n",
    "\n",
    "Every scorer might have return a value as well. e.g. the example below contains only one `ResultsHandler` that returns something - it's `MetricsDictHandler`. It returns dict containing values for the provided scorers.\n",
    "\n",
    "Metrics handling then becomes very flexible and clear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it all looks very verbose, yet it's very easy to separate all \n",
    "# the commonly used logic into one function call. \n",
    "# e.g. you may have functions that return all scorers and handlers\n",
    "# for classification, regression, imbalance learning etc.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss, roc_curve, roc_auc_score, recall_score, \\\n",
    "    precision_score\n",
    "from trava.ext.sklearn.scorers import sk, sk_proba\n",
    "from trava.ext.scorers.model_scorers import FitTimeScorer, PredictTimeScorer\n",
    "from trava.ext.results_handlers.metrics_dict import MetricsDictHandler\n",
    "from trava.ext.results_handlers.logger import LoggerHandler\n",
    "from trava.ext.results_handlers.plotter import PlotHandler\n",
    "from trava.ext.plots.roc import ROCCurvesPlotItem\n",
    "\n",
    "# LoggerHandler will log all these metrics to the console.\n",
    "logger_scorers = [\n",
    "    sk(classification_report),\n",
    "    sk(confusion_matrix),\n",
    "    sk_proba(log_loss),\n",
    "    FitTimeScorer(),\n",
    "    PredictTimeScorer()\n",
    "]\n",
    "logger_handler = LoggerHandler(scorers=logger_scorers)\n",
    "\n",
    "# PlotHandler will plot ROC AUC curve\n",
    "plot_handler = PlotHandler(plot_items=[ROCCurvesPlotItem()])\n",
    "output_scorers = [\n",
    "    sk_proba(log_loss),\n",
    "    sk_proba(roc_auc_score),\n",
    "    sk(recall_score),\n",
    "    sk(precision_score),\n",
    "    FitTimeScorer(),\n",
    "    PredictTimeScorer()\n",
    "]\n",
    "# MetricsDictHandler will return dictionary with the requested metrics.\n",
    "output_handler = MetricsDictHandler(scorers=output_scorers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits your dataset into train-test or train-test-validation parts. Each split should be configured using `DataSplitConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example of custom split result handler\n",
      "X_train shape: (398, 30)\n",
      "X_test shape: (171, 30)\n",
      "Columns:\n",
      "Index(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
      "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
      "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
      "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
      "       'smoothness error', 'compactness error', 'concavity error',\n",
      "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
      "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
      "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
      "       'worst concave points', 'worst symmetry', 'worst fractal dimension'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from trava.ext.split_logic.basic import BasicSplitLogic\n",
    "from trava.split.config import DataSplitConfig, SplitResultHandler\n",
    "from trava.split.result import SplitResult\n",
    "from trava.split.splitter import Splitter\n",
    "\n",
    "# you may additionally change ( e.g. apply resampling, or just log something like below )\n",
    "class ExampleSplitResultHandler(SplitResultHandler):\n",
    "    def __init__(self, logger: TravaLogger):\n",
    "        self._logger = logger\n",
    "    def handle(self, split_result: SplitResult) -> SplitResult:\n",
    "        self._logger.log('Example of custom split result handler')\n",
    "        self._logger.log('X_train shape: {}'.format(split_result.X_train.shape))\n",
    "        self._logger.log('X_test shape: {}'.format(split_result.X_test.shape))     \n",
    "        self._logger.log('Columns:')\n",
    "        self._logger.log(split_result.X_train.columns)\n",
    "        return split_result\n",
    "\n",
    "# we added that column just to show how ignore_cols parameter works.\n",
    "ignore_cols = ['some_redundant_col']\n",
    "split_config = DataSplitConfig(split_logic=BasicSplitLogic(shuffle=True),\n",
    "                               target_col_name='target',\n",
    "                               test_size=0.3,\n",
    "                               ignore_cols=ignore_cols,\n",
    "                               split_result_handlers=[ExampleSplitResultHandler(logger=trava_logger)])\n",
    "\n",
    "split_result = Splitter.split(df=df, config=split_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that `some_redundant_col` was ignored and not included in final split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was made with experiments tracking in mind. There are important thing you may want to track: model init parameters, metrics values etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from log_tracker import LogTracker\n",
    "from trava.trava_tracker import TravaTracker\n",
    "\n",
    "# for demonstration puproses choose from:\n",
    "\n",
    "# 1. Default - no implementation, no effect\n",
    "\n",
    "tracker = TravaTracker(scorers=[])\n",
    "\n",
    "tracker_scorers = output_scorers\n",
    "# 2. LogTracker - just logs every operation the normal tracker should perform\n",
    "# It will be quite verbose.\n",
    "# tracker = LogTracker(scorers=tracker_scorers, logger=trava_logger)\n",
    "\n",
    "# 3. MLFlowTracker - fully working tracker subclass based on MLFlow\n",
    "# Note: requires mlflow package to be installed\n",
    "# from trava.ext.tracker.mlflow import MLFlowTracker\n",
    "# tracker = MLFlowTracker(scorers=tracker_scorers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Trava"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create Trava object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trava.trava_sv import TravaSV\n",
    "\n",
    "trava = TravaSV(logger=trava_logger,\n",
    "                tracker=tracker,\n",
    "                results_handlers=[logger_handler, plot_handler, output_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to fit some models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model fit start log_reg, train shape ((398, 30)), test shape ((171, 30))\n",
      "/Users/ilya.tyutin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "Model evaluation log_reg\n",
      "*** Logging: ***\n",
      "* Results for log_reg model *\n",
      "Train metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       149\n",
      "           1       0.96      0.99      0.98       249\n",
      "\n",
      "    accuracy                           0.97       398\n",
      "   macro avg       0.97      0.96      0.97       398\n",
      "weighted avg       0.97      0.97      0.97       398\n",
      "\n",
      "confusion_matrix:\n",
      "[[140   9]\n",
      " [  3 246]]\n",
      "log_loss:\n",
      "0.09839562797036094\n",
      "\n",
      "\n",
      "Test metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        63\n",
      "           1       0.94      0.92      0.93       108\n",
      "\n",
      "    accuracy                           0.91       171\n",
      "   macro avg       0.90      0.91      0.91       171\n",
      "weighted avg       0.91      0.91      0.91       171\n",
      "\n",
      "confusion_matrix:\n",
      "[[57  6]\n",
      " [ 9 99]]\n",
      "log_loss:\n",
      "0.17052001191715083\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "fit_time:\n",
      "0.021798133850097656\n",
      "predict_time:\n",
      "0.0010569095611572266\n",
      "\n",
      "\n",
      "*** END ***\n",
      "\n",
      "../trava/ext/results_handlers/plotter.py:207: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n",
      "../trava/ext/results_handlers/plotter.py:207: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'log_reg': {'test': {'log_loss': 0.17052001191715083,\n",
       "    'roc_auc_score': 0.983392122281011,\n",
       "    'recall_score': 0.9166666666666666,\n",
       "    'precision_score': 0.9428571428571428},\n",
       "   'other': {'fit_time': 0.021798133850097656,\n",
       "    'predict_time': 0.0010569095611572266}}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "trava.fit_predict(raw_split_data=split_result,\n",
    "                  model_id='log_reg',\n",
    "                  model_type=LogisticRegression,\n",
    "                  model_init_params={'C': 1},\n",
    "                  description='First goes logistic regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see output from all the results handlers above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model fit start n.bayes, train shape ((398, 30)), test shape ((171, 30))\n",
      "Model evaluation n.bayes\n",
      "*** Logging: ***\n",
      "* Results for n.bayes model *\n",
      "Train metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93       149\n",
      "           1       0.94      0.98      0.96       249\n",
      "\n",
      "    accuracy                           0.95       398\n",
      "   macro avg       0.96      0.94      0.95       398\n",
      "weighted avg       0.95      0.95      0.95       398\n",
      "\n",
      "confusion_matrix:\n",
      "[[134  15]\n",
      " [  4 245]]\n",
      "log_loss:\n",
      "0.369845403730472\n",
      "\n",
      "\n",
      "Test metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.89        63\n",
      "           1       0.92      0.96      0.94       108\n",
      "\n",
      "    accuracy                           0.92       171\n",
      "   macro avg       0.93      0.91      0.92       171\n",
      "weighted avg       0.92      0.92      0.92       171\n",
      "\n",
      "confusion_matrix:\n",
      "[[ 54   9]\n",
      " [  4 104]]\n",
      "log_loss:\n",
      "0.697250297945911\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "fit_time:\n",
      "0.0020699501037597656\n",
      "predict_time:\n",
      "0.0011677742004394531\n",
      "\n",
      "\n",
      "*** END ***\n",
      "\n",
      "../trava/ext/results_handlers/plotter.py:207: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n",
      "../trava/ext/results_handlers/plotter.py:207: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'n.bayes': {'test': {'log_loss': 0.697250297945911,\n",
       "    'roc_auc_score': 0.9801587301587302,\n",
       "    'recall_score': 0.9629629629629629,\n",
       "    'precision_score': 0.9203539823008849},\n",
       "   'other': {'fit_time': 0.0020699501037597656,\n",
       "    'predict_time': 0.0011677742004394531}}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "trava.fit_predict(raw_split_data=split_result,\n",
    "                  model_id='n.bayes',\n",
    "                  model_type=GaussianNB,\n",
    "                  description='Then we try Gaussian Naive Bayes',\n",
    "                  keep_data_in_memory=False,\n",
    "                  keep_models_in_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine output of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*** Logging: ***\n",
      "* Results for log_reg model *\n",
      "Train metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       149\n",
      "           1       0.96      0.99      0.98       249\n",
      "\n",
      "    accuracy                           0.97       398\n",
      "   macro avg       0.97      0.96      0.97       398\n",
      "weighted avg       0.97      0.97      0.97       398\n",
      "\n",
      "confusion_matrix:\n",
      "[[140   9]\n",
      " [  3 246]]\n",
      "log_loss:\n",
      "0.09839562797036094\n",
      "\n",
      "\n",
      "Test metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        63\n",
      "           1       0.94      0.92      0.93       108\n",
      "\n",
      "    accuracy                           0.91       171\n",
      "   macro avg       0.90      0.91      0.91       171\n",
      "weighted avg       0.91      0.91      0.91       171\n",
      "\n",
      "confusion_matrix:\n",
      "[[57  6]\n",
      " [ 9 99]]\n",
      "log_loss:\n",
      "0.17052001191715083\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "fit_time:\n",
      "0.021798133850097656\n",
      "predict_time:\n",
      "0.0010569095611572266\n",
      "\n",
      "\n",
      "* Results for n.bayes model *\n",
      "Train metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93       149\n",
      "           1       0.94      0.98      0.96       249\n",
      "\n",
      "    accuracy                           0.95       398\n",
      "   macro avg       0.96      0.94      0.95       398\n",
      "weighted avg       0.95      0.95      0.95       398\n",
      "\n",
      "confusion_matrix:\n",
      "[[134  15]\n",
      " [  4 245]]\n",
      "log_loss:\n",
      "0.369845403730472\n",
      "\n",
      "\n",
      "Test metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.89        63\n",
      "           1       0.92      0.96      0.94       108\n",
      "\n",
      "    accuracy                           0.92       171\n",
      "   macro avg       0.93      0.91      0.92       171\n",
      "weighted avg       0.92      0.92      0.92       171\n",
      "\n",
      "confusion_matrix:\n",
      "[[ 54   9]\n",
      " [  4 104]]\n",
      "log_loss:\n",
      "0.697250297945911\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "fit_time:\n",
      "0.0020699501037597656\n",
      "predict_time:\n",
      "0.0011677742004394531\n",
      "\n",
      "\n",
      "*** END ***\n",
      "\n",
      "../trava/ext/results_handlers/plotter.py:207: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n",
      "../trava/ext/results_handlers/plotter.py:207: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'log_reg': {'test': {'log_loss': 0.17052001191715083,\n",
       "    'roc_auc_score': 0.983392122281011,\n",
       "    'recall_score': 0.9166666666666666,\n",
       "    'precision_score': 0.9428571428571428},\n",
       "   'other': {'fit_time': 0.021798133850097656,\n",
       "    'predict_time': 0.0010569095611572266}},\n",
       "  'n.bayes': {'test': {'log_loss': 0.697250297945911,\n",
       "    'roc_auc_score': 0.9801587301587302,\n",
       "    'recall_score': 0.9629629629629629,\n",
       "    'precision_score': 0.9203539823008849},\n",
       "   'other': {'fit_time': 0.0020699501037597656,\n",
       "    'predict_time': 0.0011677742004394531}}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trava.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we need to introduce another building block of `Trava`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FitPredictor` is responsible for fit&predict process. There is a default implementation that `Trava` uses, but you may want to subclass it to provide some custom logic. Cross-validation case is an example of the need for custom `FitPredictor`. By default there is only one fit per provided model. In case of cross-validation many fits per model are required. In that case output from every `ResultsHandler` will be averaged out ( works only for scalar metrics ). Also you are able to look at detailed results for every fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model fit start cv_decision_tree_fold_1, train shape ((99, 30)), test shape ((94, 30))\n",
      "Model evaluation cv_decision_tree_fold_1\n",
      "Model fit start cv_decision_tree_fold_2, train shape ((193, 30)), test shape ((94, 30))\n",
      "Model evaluation cv_decision_tree_fold_2\n",
      "Model fit start cv_decision_tree_fold_3, train shape ((287, 30)), test shape ((94, 30))\n",
      "Model evaluation cv_decision_tree_fold_3\n",
      "Model fit start cv_decision_tree_fold_4, train shape ((381, 30)), test shape ((94, 30))\n",
      "Model evaluation cv_decision_tree_fold_4\n",
      "Model fit start cv_decision_tree_fold_5, train shape ((475, 30)), test shape ((94, 30))\n",
      "Model evaluation cv_decision_tree_fold_5\n",
      "*** Logging: ***\n",
      "* Results for cv_decision_tree model *\n",
      "Train metrics:\n",
      "log_loss:\n",
      "0.06516081932912857\n",
      "\n",
      "\n",
      "Test metrics:\n",
      "log_loss:\n",
      "1.6501187552678729\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "fit_time:\n",
      "0.0033043861389160157\n",
      "predict_time:\n",
      "0.001035785675048828\n",
      "\n",
      "\n",
      "*** END ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'cv_decision_tree': {'test': {'log_loss': 1.6501187552678729,\n",
       "    'roc_auc_score': 0.9456332458172707,\n",
       "    'recall_score': 0.8800421351185511,\n",
       "    'precision_score': 0.956542026516009},\n",
       "   'other': {'fit_time': 0.0033043861389160157,\n",
       "    'predict_time': 0.001035785675048828}}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from trava.ext.cv.cv_fit_predictor import CVFitPredictor\n",
    "from trava.ext.cv.sk import SklearnCV\n",
    "from trava.raw_dataset import RawDataset\n",
    "\n",
    "# for cv we don't need to split data in advance so we wrap our df to pass it in Trava\n",
    "raw_dataset = RawDataset(df=df, target_col_name='target')\n",
    "cv_fit_predictor = CVFitPredictor(cv=SklearnCV(sklearn_cv=TimeSeriesSplit(n_splits=5)),\n",
    "                                  raw_dataset=raw_dataset,\n",
    "                                  ignore_cols=ignore_cols,\n",
    "                                  logger=trava_logger)\n",
    "\n",
    "trava.fit_predict(model_id='cv_decision_tree',\n",
    "                  model_type=DecisionTreeClassifier,\n",
    "                  model_init_params={'criterion': 'entropy', 'max_depth': 3},\n",
    "                  fit_predictor=cv_fit_predictor,\n",
    "                  description='Testing cross validation using decision tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics in the CV output - mean values calculated using all folds. Note that there is no plots this time. `Trava` doesn't yet support not scalar metrics averaging. But we can see performance of each fold separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*** Logging: ***\n",
      "* Results for cv_decision_tree_fold_1 model *\n",
      "Train metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        64\n",
      "           1       1.00      0.97      0.99        35\n",
      "\n",
      "    accuracy                           0.99        99\n",
      "   macro avg       0.99      0.99      0.99        99\n",
      "weighted avg       0.99      0.99      0.99        99\n",
      "\n",
      "confusion_matrix:\n",
      "[[64  0]\n",
      " [ 1 34]]\n",
      "log_loss:\n",
      "0.019288308130146814\n",
      "\n",
      "\n",
      "Test metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.91      0.74        34\n",
      "           1       0.93      0.68      0.79        60\n",
      "\n",
      "    accuracy                           0.77        94\n",
      "   macro avg       0.78      0.80      0.76        94\n",
      "weighted avg       0.82      0.77      0.77        94\n",
      "\n",
      "confusion_matrix:\n",
      "[[31  3]\n",
      " [19 41]]\n",
      "log_loss:\n",
      "3.4588653840337473\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "fit_time:\n",
      "0.0019707679748535156\n",
      "predict_time:\n",
      "0.0011017322540283203\n",
      "\n",
      "\n",
      "* Results for cv_decision_tree_fold_2 model *\n",
      "Train metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        98\n",
      "           1       1.00      0.97      0.98        95\n",
      "\n",
      "    accuracy                           0.98       193\n",
      "   macro avg       0.99      0.98      0.98       193\n",
      "weighted avg       0.98      0.98      0.98       193\n",
      "\n",
      "confusion_matrix:\n",
      "[[98  0]\n",
      " [ 3 92]]\n",
      "log_loss:\n",
      "0.04000643027374285\n",
      "\n",
      "\n",
      "Test metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.88        47\n",
      "           1       0.91      0.83      0.87        47\n",
      "\n",
      "    accuracy                           0.87        94\n",
      "   macro avg       0.88      0.87      0.87        94\n",
      "weighted avg       0.88      0.87      0.87        94\n",
      "\n",
      "confusion_matrix:\n",
      "[[43  4]\n",
      " [ 8 39]]\n",
      "log_loss:\n",
      "2.6699475827773083\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "fit_time:\n",
      "0.0024073123931884766\n",
      "predict_time:\n",
      "0.0010120868682861328\n",
      "\n",
      "\n",
      "* Results for cv_decision_tree_fold_3 model *\n",
      "Train metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       145\n",
      "           1       0.97      0.98      0.97       142\n",
      "\n",
      "    accuracy                           0.97       287\n",
      "   macro avg       0.97      0.97      0.97       287\n",
      "weighted avg       0.97      0.97      0.97       287\n",
      "\n",
      "confusion_matrix:\n",
      "[[140   5]\n",
      " [  3 139]]\n",
      "log_loss:\n",
      "0.08753460058906029\n",
      "\n",
      "\n",
      "Test metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90        24\n",
      "           1       0.99      0.94      0.96        70\n",
      "\n",
      "    accuracy                           0.95        94\n",
      "   macro avg       0.92      0.95      0.93        94\n",
      "weighted avg       0.95      0.95      0.95        94\n",
      "\n",
      "confusion_matrix:\n",
      "[[23  1]\n",
      " [ 4 66]]\n",
      "log_loss:\n",
      "0.5035501990681956\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "fit_time:\n",
      "0.003083944320678711\n",
      "predict_time:\n",
      "0.00096893310546875\n",
      "\n",
      "\n",
      "* Results for cv_decision_tree_fold_4 model *\n",
      "Train metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       169\n",
      "           1       0.94      1.00      0.97       212\n",
      "\n",
      "    accuracy                           0.96       381\n",
      "   macro avg       0.97      0.96      0.96       381\n",
      "weighted avg       0.97      0.96      0.96       381\n",
      "\n",
      "confusion_matrix:\n",
      "[[155  14]\n",
      " [  0 212]]\n",
      "log_loss:\n",
      "0.09561446492477593\n",
      "\n",
      "\n",
      "Test metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92        20\n",
      "           1       0.97      0.99      0.98        74\n",
      "\n",
      "    accuracy                           0.97        94\n",
      "   macro avg       0.96      0.94      0.95        94\n",
      "weighted avg       0.97      0.97      0.97        94\n",
      "\n",
      "confusion_matrix:\n",
      "[[18  2]\n",
      " [ 1 73]]\n",
      "log_loss:\n",
      "0.4689597520148075\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "fit_time:\n",
      "0.004142045974731445\n",
      "predict_time:\n",
      "0.0011162757873535156\n",
      "\n",
      "\n",
      "* Results for cv_decision_tree_fold_5 model *\n",
      "Train metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96       189\n",
      "           1       0.95      0.99      0.97       286\n",
      "\n",
      "    accuracy                           0.97       475\n",
      "   macro avg       0.97      0.96      0.96       475\n",
      "weighted avg       0.97      0.97      0.97       475\n",
      "\n",
      "confusion_matrix:\n",
      "[[175  14]\n",
      " [  2 284]]\n",
      "log_loss:\n",
      "0.08336029272791698\n",
      "\n",
      "\n",
      "Test metrics:\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92        23\n",
      "           1       0.99      0.96      0.97        71\n",
      "\n",
      "    accuracy                           0.96        94\n",
      "   macro avg       0.93      0.96      0.94        94\n",
      "weighted avg       0.96      0.96      0.96        94\n",
      "\n",
      "confusion_matrix:\n",
      "[[22  1]\n",
      " [ 3 68]]\n",
      "log_loss:\n",
      "1.1492708584453053\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "fit_time:\n",
      "0.00491786003112793\n",
      "predict_time:\n",
      "0.0009799003601074219\n",
      "\n",
      "\n",
      "*** END ***\n",
      "\n",
      "../trava/ext/results_handlers/plotter.py:207: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n",
      "../trava/ext/results_handlers/plotter.py:207: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'cv_decision_tree_fold_1': {'test': {'log_loss': 3.4588653840337473,\n",
       "    'roc_auc_score': 0.8963235294117646,\n",
       "    'recall_score': 0.6833333333333333,\n",
       "    'precision_score': 0.9318181818181818},\n",
       "   'other': {'fit_time': 0.0019707679748535156,\n",
       "    'predict_time': 0.0011017322540283203}},\n",
       "  'cv_decision_tree_fold_2': {'test': {'log_loss': 2.6699475827773083,\n",
       "    'roc_auc_score': 0.9137618832050702,\n",
       "    'recall_score': 0.8297872340425532,\n",
       "    'precision_score': 0.9069767441860465},\n",
       "   'other': {'fit_time': 0.0024073123931884766,\n",
       "    'predict_time': 0.0010120868682861328}},\n",
       "  'cv_decision_tree_fold_3': {'test': {'log_loss': 0.5035501990681956,\n",
       "    'roc_auc_score': 0.9699404761904762,\n",
       "    'recall_score': 0.9428571428571428,\n",
       "    'precision_score': 0.9850746268656716},\n",
       "   'other': {'fit_time': 0.003083944320678711,\n",
       "    'predict_time': 0.00096893310546875}},\n",
       "  'cv_decision_tree_fold_4': {'test': {'log_loss': 0.4689597520148075,\n",
       "    'roc_auc_score': 0.9726351351351352,\n",
       "    'recall_score': 0.9864864864864865,\n",
       "    'precision_score': 0.9733333333333334},\n",
       "   'other': {'fit_time': 0.004142045974731445,\n",
       "    'predict_time': 0.0011162757873535156}},\n",
       "  'cv_decision_tree_fold_5': {'test': {'log_loss': 1.1492708584453053,\n",
       "    'roc_auc_score': 0.9755052051439069,\n",
       "    'recall_score': 0.9577464788732394,\n",
       "    'precision_score': 0.9855072463768116},\n",
       "   'other': {'fit_time': 0.00491786003112793,\n",
       "    'predict_time': 0.0009799003601074219}}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trava.detailed_results_for(model_id='cv_decision_tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate after fit_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate metrics after `fit_predict` call. If you noticed, when we fit `GaussianNB` model, we passed two extra parameters: `keep_data_in_memory=False`, `keep_models_in_memory=False`. Those parameters are responsible for memory saving. Yet we still can calculate new metrics, despite there is no model and data in memory. \n",
    "\n",
    "Note: it's not true for some complex scorers, see Scorer class for details, parameters `requires_raw_model` and `requires_X_y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*** Logging: ***\n",
      "* Results for n.bayes model *\n",
      "Train metrics:\n",
      "f1_score:\n",
      "0.962671905697446\n",
      "\n",
      "\n",
      "Test metrics:\n",
      "f1_score:\n",
      "0.9411764705882353\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "\n",
      "\n",
      "*** END ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "trava.evaluate(model_id='n.bayes',\n",
    "               results_handlers=[LoggerHandler(scorers=[sk(f1_score)])],\n",
    "               save_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We passed `save_results=False` so we just took a look at the value. If we pass `True` - it will be recorded and then visible by calling `trava.results`. There is `evaluate_track` method, works the same, but for the tracker."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
